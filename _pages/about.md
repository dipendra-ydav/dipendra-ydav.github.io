---
permalink: /
title: "Dipendra Yadav"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a software engineer in the DevOps team at [PlanetAI](https://planet-ai.de/) under the supervision of [Stefan Gerke](https://www.linkedin.com/in/stefan-gerke-672830157/).
I am broadly interested in transfer learning, self-supervised learning, domain adaptation and low-resource machine translation, in both supervised and unsupervised scenarios. My PhD thesis is currently focused on improving cross-lingual pretraining for low-resource languages. In this line of work, I proposed a curriculum for pretraining masked language models to improve unsupervised translation for high-resource/low-resource language pairs. I have also worked on enhancing cross-lingual pretraining by including lexical information from non-contextualized embeddings. Lately, I have been exploring methods of improving pretraining via denoising auto-encoding leveraging typology information, with the final goal of achieving better translation quality. 

 Before that, I completed my Master at the [Universit√§t Rostock](https://www.uni-rostock.de/en/) in Rostock, Germany, department of [Electrical and Computer Engineering](https://www.ief.uni-rostock.de/en/). In my thesis, I explored ways of improving transfer learning with language Modeling for several classification tasks, most notably on emotion recognition, under the supervision of [Alex Potamianos](https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en).
